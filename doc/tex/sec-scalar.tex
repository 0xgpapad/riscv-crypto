
As per the RISC-V Cryptographic Extensions Task Group charter:
``{\em The committee will also make ISA extension proposals for lightweight
scalar instructions for 32 and 64 bit machines that improve the performance
and reduce the code size required for software execution of common algorithms
like AES and SHA and lightweight algorithms like PRESENT and GOST}".

\bigskip

For context, some these instructions have been developed based on academic
work at the University of Bristol as part of the XCrypto project
\cite{MPP:19},
and work by
Paris Telecom on acceleration of lightweight block ciphers
\cite{TGMGD:19}.

% ============================================================================

\subsection{Shared Bitmanip Extension Functionality}

Many of the primitive operations used in symmetric key cryptography
and cryptographic hash functions are well supported by the
RISC-V Bitmanip \cite{riscv:bitmanip:repo} extension
\footnote{
At the time of writing, the Bitmanip extension is still undergoing
standardization.
Please refer to the bitmanip draft specification
\cite{riscv:bitmanip:draft}
directly for the
latest information, as it may be slightly ahead of what is described
here.
}.
We propose that the scalar cryptographic extension {\em reuse} a
subset of the instructions from the Bitmanip extension directly.
Specifically, this would mean that
a core implementing
{\em either}
the scalar cryptographic extensions,
{\em or}
the bitmanip extension,
{\em or}
both,
would be able to depend on the existence of these instructions.

The following subsections give the assembly syntax of instructions
proposed for inclusion in the scalar crypto extension, along with a
set of use-cases for common algorithms or primitive operations.
For information on the semantics of the instructions, we refer directly
to the bitmanip draft specification.

\subsubsection{Rotations}

\begin{isa}
RV32, RV64:
    ror    rd, rs1, rs2
    rol    rd, rs1, rs2
    rori   rd, rs1, imm

RV64 only:
    rorw   rd, rs1, rs2
    rolw   rd, rs1, rs2
    roriw  rd, rs1, imm
\end{isa}

See \cite[Section 3.1.1]{riscv:bitmanip:draft} for exact details of
these instructions.
Standard bitwise rotation is a primitive operation in many block ciphers and
hash functions.
It particularly features in the ARX (Add,Rotate,Xor) class of
block ciphers
\footnote{\url{https://www.cosic.esat.kuleuven.be/ecrypt/courses/albena11/slides/nicky_mouha_arx-slides.pdf}}.

\begin{isa}
RV32, RV64:
    fsl   rd, rs1, rs3, rs2
    fsr   rd, rs1, rs3, rs2
    fsri  rd, rs1, rs3, imm

RV64 only:
    fslw  rd, rs1, rs3, rs2
    fsrw  rd, rs1, rs3, rs2
    fsriw rd, rs1, rs3, imm
\end{isa}

See \cite[Section 2.9.3]{riscv:bitmanip:draft} for exact details of
these instructions.
The {\em funnel shift} instructions create a $2*XLEN$ word by
concatenating {\tt rs1} and {\tt rs3}, which is then
left/right rotate shifted by the amount in {\tt imm}/{\tt rs2}.
These are useful for implementing double-width rotations.

\todo{Useful for: SHA3/SHA512 on RV32. 128 bit rotations aren't common, so
possibly only recommend the RV32 instructions?}


\subsubsection{Other Permutations: {\tt grev} and {\tt shfl}}

\begin{isa}
RV32, RV64:
    grev rd, rs1, rs2
    grevi rd, rs1, imm

RV64 only:
    grevw rd, rs1, rs2
    greviw rd, rs1, imm
\end{isa}

The Generalized Reverse ({\tt grev*}) instructions can be used for 
``{\em byte-order swap, bitwise reversal, short-order-swap,
word-order-swap (RV64), nibble-order swap, bitwise reversal in a byte}".
These operations are useful for various permutation operations
needed either by block ciphers and hash-functions directly, or for
endianness correction of data.
Endianness correction is important because
cryptography often occurs in the context of communication, which requires
standardized endianness which may be different from the natural machine
endianness.

\todo{Specific use-cases for grev.}

\begin{isa}
RV32, RV64:
    shfl    rd, rs1, rs2
    unshfl  rd, rs1, rs2
    shfli   rd, rs1, rs2
    unshfli rd, rs1, rs2

RV64:
    shflw   rd, rs1, rs2
    unshflw rd, rs1, rs2
\end{isa}

The generalized shuffle instructions are useful for implementing
generic bit permutation operations.
Algorithms such as 
DES \footnote{
One might reasonably argue that given the heritage of DES, it's support
shouldn't really be any sort of consideration for a forward looking
ISA like RISC-V.
}
and
PRESENT\cite{block:present} with
irregular / odd permutations are most-likely to benefit from this
instruction.

\todo{More research needed on specific algorithms / use-cases for
these instructions. They are included as ``hypothetically useful"
at the moment.}

\subsubsection{Carry-less Multiply}

\begin{isa}
RV32, RV64:
    clmul rd, rs1, rs2
    clmulh rd, rs1, rs2
    clmulr rd, rs1, rs2

RV64 only:
    clmulw rd, rs1, rs2
    clmulhw rd, rs1, rs2
    clmulrw rd, rs1, rs2
\end{isa}

See \cite[Section 2.6]{riscv:bitmanip:draft} for exact details of
this instruction.
As is mentioned there, obvious cryptographic use-cases for carry-less
multiply are for Galois Counter Mode (GCM) block cipher operations
\footnote{\url{https://en.wikipedia.org/wiki/Galois/Counter_Mode}}.
GCM is recommended by NIST as a block cipher mode of operation
\cite{nist:gcm}.

\subsubsection{Conditional Move}

\begin{isa}
RV32, RV64:
    cmov rd, rs2, rs1, rs3
\end{isa}

See \cite[Section 2.9.2]{riscv:bitmanip:draft} for exact details of
this instruction.
Conditional move is useful for implementing constant-time cryptographic
code and avoiding control flow changes.

\subsubsection{Logic With Negate}

\begin{isa}
RV32, RV64:
    andn rd, rs1, rs2
     orn rd, rs1, rs2
    xorn rd, rs1, rs2
\end{isa}

See \cite[Section 2.1.3]{riscv:bitmanip:draft} for exact details of
these instructions.
These instructions are useful inside hash functions, block ciphers and
for implementing software based side-channel countermeasures like masking.

\todo{Useful for: SHA3 / SHA2 / Masking}

\subsubsection{Packing}

\begin{isa}
RV32, RV64: 
    pack   rd, rs1, rs2
    packu  rd, rs1, rs2
    packh  rd, rs1, rs2

RV64: 
    packw  rd, rs1, rs2
    packuw rd, rs1, rs2
\end{isa}

See \cite[Section 2.1.4]{riscv:bitmanip:draft} for exact details of
these instructions.
Some lightweight block ciphers (e.g. PRINCE \cite{block:prince}) use
sub-word data types in their primitives.
The bitmanip pack instructions are useful for performing rotations on
16-bit data elements.
They are also useful for re-arranging halfwords within words, and
generally getting data into the right shape prior to applying transforms.

\todo{Concrete list of use-cases.}


% ============================================================================

\subsection{LUT4 Instruction}

\begin{isa}
RV32, RV64:
    lut4    rd, rs1, rs2        // Variant 1
    lut4    rd, rs1, rs2 rs3    // Variant 2
\end{isa}

\begin{lstlisting}[language=c]
// C code for lut4 variant 1 on RV32.
uint32_t lut4_rv32_v1 (uint32_t rd, uint32_t rs1, uint32_t rs2) {
    uint32_t result = 0;
    uint64_t lut    = (((uint64_t)rs1) << 32) | rs2;
    for(int i = 0; i < XLEN;  i += 4) {
        uint32_t in     = ((rd  >>  i) & 0xF) << 2;
        uint64_t toadd  = ((lut >> in) & 0xF) << i;
        result         |= toadd;
    }
    return result;
}
\end{lstlisting}

The {\tt lut4} instruction implements a 4-bit lookup table operation
on every nibble in a source word.
The lookup table is constructed by concatenating two 32-bit words.
On RV32, two XLEN registers are concatenated to form the LUT.
On RV64, the low 32-bits of two XLEN registers are concatenated to form
the LUT.
The instruction is extremely useful for any lightweight block-cipher
which uses a 4-bit SBox.
A comprehensive list and assessment of such ciphers can be found
in \cite{TGMGD:19}.

Two variants are suggested on the assumption that only one variant is
proposed for standardization:
\begin{itemize}
\item Variant 1: Concatenate {\tt rs1} and {\tt rs2} to form the LUT.
    Use {\tt rd} as a source/destination register.
    The contents of {\tt rd} is chunked into nibbles, which are then
    used as inputs to the lut. The corresponding nibble is replaced
    with the output of the lut and written back to {\tt rd}.
\item Variant 2: Identical to variant 1, but without the read/overwrite
    semantics for {\tt rd}. Input to the lut is taken from {\tt rs3}
    and the output is written to {\tt rd}.
\end{itemize}
Variant 1 is obviously more efficient in terms of encoding space.
Variant 2 has the advantage from a side-channel perspective
of not over-writing it's source register with the result.
This is useful as it avoids implicit hamming distance leakage in
a correlation power analysis (CPA) context. This could be mitigated
on a micro-architectural level however
\footnote{
    How best to support software side-channel countermeasures at the
    ISA level of abstraction is a non-trivial open question/problem.
}.

% ============================================================================

\subsection{Multi-precision Arithmetic}

Multi-precision arithmetic is commonly used in public key cryptography.
RISC-V struggles with long arithmetic for two reasons:

\begin{itemize}
\item A lack of carry / overflow detection. This hinders the arithmetic
    side of implementing multi-precision arithmetic.

\item A lack of indexed load and store instructions.
    Multi-precision arithmetic typically involves stepping through
    at-least three different arrays at different indices: two operands
    and a result.
    This results in a lot of pointer arithmetic at the end of loops
    on RISC-V.
\end{itemize}


\subsubsection{Multi-precision Multiply Accumulate Unsigned}

\begin{isa}
RV32, RV64:
    mmulu   rdp, rs1, rs2, rs3
\end{isa}

\begin{lstlisting}[]
// Equivalent RV32IM / RV64IM assembly code listing. rdp = (rd2,rd1)
mul     t1  , rs1, rs2  //       t1   = low(rs1 * rs2)
mulhu   rd2 , rs1, rs2  // (rd2, t1)  =     rs1 * rs2
add     rd1 , t1 , rs3  //      rd1   = low(rs1 * rs2) + rs3
sltu    t1  , rd1, t1   //       t1   = rd1 < t1
add     rd2 , rd2, rs2  //      rd2  += t1
\end{lstlisting}

The \mnemonic{umacc} instruction performs an unsigned multiply
of two \XLEN sources, forming a $2*$\XLEN result.
The third \XLEN source register is then added to the $2*$\XLEN result.
The $2*$\XLEN result is then written back to an odd-even register
pair \rdp.


\subsubsection{Multi-precision Accumulate Unsigned}

\begin{isa}
RV32, RV64:
    maccu   rdp, rs1, rs2, rs3
\end{isa}

\begin{lstlisting}[]
// Equivalent RV32IM / RV64IM assembly code listing. rdp = (rd2,rd1)
// 
add     rd1, rs2, rs3
sltu    rs1, rd1, rs2
add     rd2, rs2, rs1   // (rd1,rd2) = (rs1,rs2)+rs3
\end{lstlisting}

% ============================================================================

\subsection{Lightweight AES Acceleration}

\begin{isa}
RV32, RV64:
    saes.sbox.enc rd, rs1
    saes.sbox.dec rd, rs2
\end{isa}

These instructions implement the 
{\tt SubBytes} \cite[Section 5.1.1]{nist:fips:197}
and
{\tt InvSubBytes} \cite[Section 5.3.1]{nist:fips:197}
steps of the AES Block Cipher \cite{nist:fips:197}.
The low 32-bits of {\tt rs1} are split into bytes.
Each byte has the relevant transformation applied, before
being written back to the corresponding byte position in {\tt rd}.
On an RV64 platform, the high 32-bits of the result are zero
extended.

\note{The AES SBox operations are very popular targets for
power or EM based side-channel attacks.
While an instruction like this allows for very efficient AES
implementations, it is very difficult to implement the operation
in a countermeasure friendly way.}

\question{What should the Task Group policy be on enabling
software and/or hardware based side-channel countermeasure?}

\question{Given that enabling and optimizing side-channel countermeasures 
is so hard, should the TG focus instead on code-density and performance?
In this case, can implementations with much more structured
inputs (as in \cite{TG:06}) be considered.}

% ============================================================================

\subsection{Lightweight SHA2 Acceleration}

\begin{isa}
RV32, RV64:
    ssha256.s0 rd, rs1 : rd = ror32(rs1, 7) ^ ror32(rs1, 18) ^ srl32(rs1, 3)
    ssha256.s1 rd, rs1 : rd = ror32(rs1,17) ^ ror32(rs1, 19) ^ srl32(rs1,10)
    ssha256.s2 rd, rs1 : rd = ror32(rs1, 2) ^ ror32(rs1, 13) ^ ror32(rs1,22)
    ssha256.s3 rd, rs1 : rd = ror32(rs1, 6) ^ ror32(rs1, 11) ^ ror32(rs1,25)
\end{isa}

The {\tt ssha256.sX}
instructions implement the core of the four sigma and sum functions used in
the SHA256 hash function \cite[Section 4.1.2]{nist:fips:180:4}.
These operations will be supported for a both RV32 and RV64 targets.
For RV32, the entire XLEN source register is operated on.
For RV64, the low 32-bits of the XLEN register are read and operated on,
with the result zero extended to XLEN bits.
Though named for SHA256, the instructions work for both the
SHA-224 and SHA-256 parameterizations as described in
\cite{nist:fips:180:4}.


\begin{isa}
RV64:
    ssha512.s0 rd, rs1 : rd = ror64(rs1, 1) ^ ror64(rs1,  8) ^ srl64(rs1, 7)
    ssha512.s1 rd, rs1 : rd = ror64(rs1,19) ^ ror64(rs1, 61) ^ srl64(rs1, 6)
    ssha512.s2 rd, rs1 : rd = ror64(rs1,28) ^ ror64(rs1, 34) ^ ror64(rs1,39)
    ssha512.s3 rd, rs1 : rd = ror64(rs1,14) ^ ror64(rs1, 18) ^ ror64(rs1,41)
\end{isa}

The {\tt ssha512.sX}
instructions implement the core of the four sigma and sum functions used in
the SHA512 hash function \cite[Section 4.1.3]{nist:fips:180:4}.
These operations will be supported for RV64 targets only.
Though named for the SHA-512 parameterization, the instructions
can be used for all of the SHA-384, SHA-512, SHA-512/224 and SHA-512/256
parameterizations as described in \cite{nist:fips:180:4}.

\note{
The remaining two core functions which make up the SHA256/512
hash functions are the $Ch$ and $Maj$ functions:
\begin{itemize}
\item \lstinline{Ch(x,y,z)  = (x & y) ^ (~x & z)}
\item \lstinline{Maj(x,y,z) = (x & y) ^ ( x & z) ^ ( y & z )}
\end{itemize}
As ternary functions, they are much too expensive in terms of
opcode space to consider for inclusion as dedicated instructions for
such a specialist use case.
They are amenable however to macro-op fusion on cores which implement it.
}

\todo{
These instructions give a
$\approx 1.8x$ performance improvement
and
$\approx 0.6x$ static code size improvement
to a vanilla SHA256 implementation on RV32.
This is based on the work for \cite{MPP:19}, which blends results
from other experimental instructions.
A dedicated benchmark for investigating the effect of just these instructions
for the standardization process will be needed.
}

% ============================================================================

\subsection{Lightweight SHA3 Acceleration}

\begin{isa}
RV32, RV64:
    ssha3.xy rd, rs1, rs2 : rd = (( rs1    % 5) + 5(           rs2 % 5)) << 3
    ssha3.x1 rd, rs1, rs2 : rd = (((rs1+1) % 5) + 5(           rs2 % 5)) << 3
    ssha3.x2 rd, rs1, rs2 : rd = (((rs1+2) % 5) + 5(           rs2 % 5)) << 3
    ssha3.x4 rd, rs1, rs2 : rd = (((rs1+4) % 5) + 5(           rs2 % 5)) << 3
    ssha3.yx rd, rs1, rs2 : rd = (( rs2    % 5) + 5((2*rs1 + 3*rs2)% 5)) << 3
\end{isa}

These instructions accelerate code-dense implementations of the SHA3 secure
hash function \cite{nist:fips:202}.
They work on the low $3$ bits of the input {\tt rs1} and {\tt rs2} registers,
and compute indices into the state array of the round function.
They are designed to replace both canonical implementations of the
{\tt index} function and lookup table based implementations.
In both cases, these instructions offer substantial
improvements in performance, static code size and dynamic instruction
bandwidth.
A longer discussion of the merits of these instructions can
be found in Appendix \ref{sec:appendix:sha3}.

% ============================================================================

\subsection{Micro-architectural Recommendations}

\todo{Macro-op fusion suggestions, side-channel considerations.}

% ============================================================================

