
As per the RISC-V Cryptographic Extensions Task Group charter:
``{\em The committee will also make ISA extension proposals for lightweight
scalar instructions for 32 and 64 bit machines that improve the performance
and reduce the code size required for software execution of common algorithms
like AES and SHA and lightweight algorithms like PRESENT and GOST}".

\bigskip

For context, some these instructions have been developed based on academic
work at the University of Bristol as part of the XCrypto project
\cite{MPP:19},
and work by
Paris Telecom on acceleration of lightweight block ciphers
\cite{TGMGD:19}.

\question{
Implementation Diversity.
There are often many different ways of implementing certain
standardised cryptographic algorithms (AES T-Table v.s. packed for example).
Given that different implementation styles can have security implications
as well, what should the TG policy be on enabling implementation diversity?
By encouraging a particular style of implementation, more specific and
light-weight instructions can be defined to accelerate specifically that
implementation (See \cite{TG:06}).
This may come with flexibility costs, and possibly limit implementers
choices in terms of security and attack countermeasures.
}

\question{
Chicken and Egg Problems. While security must always be the first design
criteria for a new cipher, cryptographers can weight their choice of
primitives by how well they are supported by existing micro-processors.
If suddenly a new primitive becomes well supported by new
architectures, how does this impact cipher design?
Is this a motivation to include some level of generally / speculatively
useful stuff?
See \cite{block:salsa20, LSYRR:04}.
Likewise, criteria for ``lightweight" ciphers might include suitability
for hardware.
This might mean narrow data widths (e.g. SPARX \cite{DPUVGB:16})
which though excellent for hardware, are less than a CPU word-width,
making operations like addition, shifts and rotate {\em more}
awkward, not less.
}

% ============================================================================

\subsection{Shared Bitmanip Extension Functionality}

Many of the primitive operations used in symmetric key cryptography
and cryptographic hash functions are well supported by the
RISC-V Bitmanip \cite{riscv:bitmanip:repo} extension
\footnote{
At the time of writing, the Bitmanip extension is still undergoing
standardisation.
Please refer to the Bitmanip draft specification
\cite{riscv:bitmanip:draft}
directly for the
latest information, as it may be slightly ahead of what is described
here.
}.
We propose that the scalar cryptographic extension {\em reuse} a
subset of the instructions from the Bitmanip extension directly.
Specifically, this would mean that
a core implementing
{\em either}
the scalar cryptographic extensions,
{\em or}
the Bitmanip extension,
{\em or}
both,
would be able to depend on the existence of these instructions.

The following subsections give the assembly syntax of instructions
proposed for inclusion in the scalar crypto extension, along with a
set of use-cases for common algorithms or primitive operations.
For information on the semantics of the instructions, we refer directly
to the Bitmanip draft specification.

\subsubsection{Rotations}

\begin{cryptobitmanipisa}
RV32, RV64:
    ror    rd, rs1, rs2
    rol    rd, rs1, rs2
    rori   rd, rs1, imm

RV64 only:
    rorw   rd, rs1, rs2
    rolw   rd, rs1, rs2
    roriw  rd, rs1, imm
\end{cryptobitmanipisa}

See \cite[Section 3.1.1]{riscv:bitmanip:draft} for exact details of
these instructions.
Standard bitwise rotation is a primitive operation in many block ciphers and
hash functions.
It particularly features in the ARX (Add,Rotate,Xor) class of
block ciphers
\footnote{\url{https://www.cosic.esat.kuleuven.be/ecrypt/courses/albena11/slides/nicky_mouha_arx-slides.pdf}}.

Algorithms making use of 32-bit rotations:
SHA256, AES (Shift Rows), ChaCha20

Algorithms making use of 64-bit rotations:
SHA512, SHA3

\begin{cryptobitmanipisa}
RV32 only:
    fsl   rd, rs1, rs3, rs2
    fsr   rd, rs1, rs3, rs2
    fsri  rd, rs1, rs3, imm
\end{cryptobitmanipisa}

See \cite[Section 2.9.3]{riscv:bitmanip:draft} for exact details of
these instructions.
The {\em funnel shift} instructions create a $2*$\XLEN word by
concatenating {\tt rs1} and {\tt rs3}, which is then
left/right rotate shifted by the amount in {\tt imm}/{\tt rs2}.
These are useful for implementing double-width rotations.
There are (currently) no examples of widely used algorithms which
use anything larger than a 64-bit rotation, hence the funnel shift
instructions are only recommended for inclusion on RV32, since RV64
will support 64-bit rotations naturally.

Algorithms using 64-bit rotations:
SHA512,
SHA3\footnote{SHA3 can avoid awkward double-width rotations using a
technique called "Bit Interleaving". Benchmarking will be needed to
see if this technique is a worthy mitigation for removing the funnel shifts.}

\subsubsection{Other Permutations: {\tt grev} and {\tt shfl}}

\begin{cryptobitmanipisa}
RV32, RV64:
    grev rd, rs1, rs2
    grevi rd, rs1, imm

RV64 only:
    grevw rd, rs1, rs2
    greviw rd, rs1, imm
\end{cryptobitmanipisa}

The Generalised Reverse ({\tt grev*}) instructions can be used for 
``{\em byte-order swap, bitwise reversal, short-order-swap,
word-order-swap (RV64), nibble-order swap, bitwise reversal in a byte}".
These operations are useful for various permutation operations
needed either by block ciphers and hash-functions directly, or for
endianness correction of data.
Endianness correction is important because
cryptography often occurs in the context of communication, which requires
standardised endianness which may be different from the natural machine
endianness.

\todo{Specific use-cases for grev.}

\begin{cryptobitmanipisa}
RV32, RV64:
    shfl    rd, rs1, rs2
    unshfl  rd, rs1, rs2
    shfli   rd, rs1, rs2
    unshfli rd, rs1, rs2

RV64:
    shflw   rd, rs1, rs2
    unshflw rd, rs1, rs2
\end{cryptobitmanipisa}

The generalised shuffle instructions are useful for implementing
generic bit permutation operations.
Algorithms such as 
DES \footnote{
One might reasonably argue that given the heritage of DES, it's support
shouldn't really be any sort of consideration for a forward looking
ISA like RISC-V.
}
and
PRESENT\cite{block:present} with
irregular / odd permutations are most-likely to benefit from this
instruction.

\todo{More research needed on specific algorithms / use-cases for
these instructions. They are included as ``hypothetically useful"
at the moment.}

\subsubsection{Carry-less Multiply}

\begin{cryptobitmanipisa}
RV32, RV64:
    clmul rd, rs1, rs2
    clmulh rd, rs1, rs2
    clmulr rd, rs1, rs2

RV64 only:
    clmulw rd, rs1, rs2
    clmulhw rd, rs1, rs2
    clmulrw rd, rs1, rs2
\end{cryptobitmanipisa}

See \cite[Section 2.6]{riscv:bitmanip:draft} for exact details of
this instruction.
As is mentioned there, obvious cryptographic use-cases for carry-less
multiply are for Galois Counter Mode (GCM) block cipher operations
\footnote{\url{https://en.wikipedia.org/wiki/Galois/Counter_Mode}}.
GCM is recommended by NIST as a block cipher mode of operation
\cite{nist:gcm}.

\subsubsection{Conditional Move}

\begin{cryptobitmanipisa}
RV32, RV64:
    cmov rd, rs2, rs1, rs3
\end{cryptobitmanipisa}

See \cite[Section 2.9.2]{riscv:bitmanip:draft} for exact details of
this instruction.
Conditional move is useful for implementing constant-time cryptographic
code and avoiding control flow changes.

\subsubsection{Logic With Negate}

\begin{cryptobitmanipisa}
RV32, RV64:
    andn rd, rs1, rs2
     orn rd, rs1, rs2
    xorn rd, rs1, rs2
\end{cryptobitmanipisa}

See \cite[Section 2.1.3]{riscv:bitmanip:draft} for exact details of
these instructions.
These instructions are useful inside hash functions, block ciphers and
for implementing software based side-channel countermeasures like masking.

Useful for: SHA3 / SHA2 / Masking

\subsubsection{Packing}

\begin{cryptobitmanipisa}
RV32, RV64: 
    pack   rd, rs1, rs2
    packu  rd, rs1, rs2
    packh  rd, rs1, rs2

RV64: 
    packw  rd, rs1, rs2
    packuw rd, rs1, rs2
\end{cryptobitmanipisa}

See \cite[Section 2.1.4]{riscv:bitmanip:draft} for exact details of
these instructions.
Some lightweight block ciphers
(e.g. SPARX \cite{DPUVGB:16})
use sub-word data types in their primitives.
The Bitmanip pack instructions are useful for performing rotations on
16-bit data elements.
They are also useful for re-arranging halfwords within words, and
generally getting data into the right shape prior to applying transforms.

Algorithms with sub-word rotations/shifts:
SPARX

% ============================================================================

\subsection{LUT4 Instruction}

\begin{cryptoisa}
lut4lo  rd, rs1, rs2
    for i = 0..7 : rd.4[i] = rs2.4[rs1.4[i]&0x7] if (rs1.4[i] <  8) else 0

lut4hi  rd, rs1, rs2
    for i = 0..7 : rd.4[i] = rs2.4[rs1.4[i]&0x7] if (rs1.4[i] >= 8) else 0

lut4    rd, rs1, rs2
    for i = 0..15 : rd.4[i] = rs2.4[rs1.4[i]]
\end{cryptoisa}

The \mnemonic{lut4*} instructions are used to implement 4-bit lookup tables
on every nibble in a source word.
Many lightweight block ciphers use 4x4 SBoxes:
PRINCE\cite{block:prince},
PRESENT\cite{block:present},
Rectangle\cite{block:rectangle},
GIFT\cite{block:gift},
Twine\cite{block:twine},
Skinny, MANTIS\cite{block:skinny},
Midori \cite{block:midori}.

On RV32, the lookup step is split into two stages.
The \mnemonic{lut4lo} instruction
updates nibbles in the destination with the looked-up value
iff the index is less than eight.
The \mnemonic{lut4hi} version does
the same for index values greater than/equal to eight.
The results can then be or'd together.
An example implementation of an $8$-nibble parallel SBox using these
instructions is found in \figref{example:lut4:1}

On RV64, the entire set of LUT elements fits in a single source register.
The RV64 only \mnemonic{lut4} instruction stores the entire lut in
\rstwo, and uses each nibble in \rsone as an index into it.

Equivilent C code listings for the instructions are found in
\figref{equiv:c:lut4}.

%\begin{figure}
%\begin{lstlisting}[style=compact]
%lut4lo:
%lut4hi:
%\end{lstlisting}
%\caption{
%    High level description of the 
%    \mnemonic{lut4lo} and \mnemonic{lut4hi} instructions.
%    Here \xreg{rd}, \xreg{rs1} and \xreg{rs2} are 32-bit wide registers.
%    When executing on RV64, the results are \zeroextended to \XLEN bits.
%}
%\end{figure}

\begin{figure}
\begin{lstlisting}[style=ASM]
sbox_4bit:
    lut4lo  a3, a0, a1      // a0 = indexes, a1 = low  8 LUT nibbles
    lut4hi  a4, a0, a2      // a0 = indexes, a2 = high 8 LUT nibbles
    or      a0, a3, a4      // Or results together.
    ret                     // Function Return
\end{lstlisting}
\caption{
    Implement 8 parallel 4-to-4 bit SBox operations on RV32 using
    the \mnemonic{lut4hi} and \mnemonic{lut4lo} instructions.
    The Inputs to the SBox are stored in \xreg{a0}.
    The high and low $8$ elements of the
    LUT are stored in \xreg{a2} and \xreg{a1} respectivley.
}
\label{fig:example:lut4:1}
\end{figure}

\begin{figure}
\begin{lstlisting}[style=C]
uint32_t lut4_lo (uint32_t rs1, uint32_t rs2) {
    uint32_t result;
    for(int i = 0; i < 32; i += 4) {
        uint8_t idx =  (rs1 >> i) & 0x7;
        uint8_t lo  = ((rs1 >> i) & 0xF) < 8;
        if(lo) { result |= ((rs2 >> (4*idx) & 0xF) << i; }
    }
    return result;
}
uint32_t lut4_hi (uint32_t rs1, uint32_t rs2) {
    uint32_t result;
    for(int i = 0; i < 32; i += 4) {
        uint8_t idx =  (rs1 >> i) & 0x7;
        uint8_t hi  = ((rs1 >> i) & 0xF) > 8;
        if(hi) { result |= ((rs2 >> (4*idx) & 0xF) << i; }
    }
    return result;
}
uint64_t lut4 (uint64_t rs1, uint64_t rs2) {
    uint64_t result;
    for(int i = 0; i < 64; i += 4) {
        result |= ((rs2 >> (4*((rs1 >> i)&0xF)) & 0xF) << i;
    }
    return result;
}
\end{lstlisting}
\caption{
    Equivilent C code models for the
    \mnemonic{lut4lo}, \mnemonic{lut4hi} and \mnemonic{lut4}
    instructions.
}
\label{fig:equiv:c:lut4}
\end{figure}

% ============================================================================

\subsection{Multi-precision Arithmetic}

Multi-precision arithmetic is commonly used in public key cryptography.
RISC-V struggles with long arithmetic for two reasons:

\begin{itemize}
\item A lack of carry / overflow detection. This hinders the arithmetic
    side of implementing multi-precision arithmetic.

\item A lack of indexed load and store instructions.
    Multi-precision arithmetic typically involves stepping through
    at-least three different arrays at different indices: two operands
    and a result.
    This results in a lot of pointer arithmetic at the end of loops
    on RISC-V.
\end{itemize}

\question{
These instructions introduce the ``double width write-back" idiom to
RISC-V.
How acceptable is this?
Which other instructions could take advantage of this?
How well does this overlap with what the P (DSP) extension task group
is proposing?
}

\todo{
Benchmarking flow for these instructions based on
long-multiply / modular exponentiation.
}

\subsubsection{Multi-precision Multiply Accumulate Unsigned}

\begin{cryptoisa}
RV32, RV64:
    mmulu   rdp, rs1, rs2, rs3
\end{cryptoisa}

\begin{lstlisting}[]
// Equivalent RV32IM / RV64IM assembly code listing. rdp = (rd2,rd1)
mul     t1  , rs1, rs2  //       t1   = low(rs1 * rs2)
mulhu   rd2 , rs1, rs2  // (rd2, t1)  =     rs1 * rs2
add     rd1 , t1 , rs3  //      rd1   = low(rs1 * rs2) + rs3
sltu    t1  , rd1, t1   //       t1   = rd1 < t1
add     rd2 , rd2, rs2  //      rd2  += t1
\end{lstlisting}

The \mnemonic{mmulu} instruction performs an unsigned multiply
of two \XLEN sources, forming a $2*$\XLEN result.
The third \XLEN source register is then added to the $2*$\XLEN result.
The $2*$\XLEN result is then written back to an odd-even register
pair \rdp.

\note{
This instruction could be cracked into two pieces to avoid the
double width write-back idiom.
This would realise two instructions
(\mnemonic{mmulu} and \mnemonic{mmuluh})
which yield the high and low parts of the full $2*$\XLEN result.
They could then be fused in fancier micro-architectures.
}

\subsubsection{Multi-precision Accumulate Unsigned}

\begin{cryptoisa}
RV32, RV64:
    maccu   rdp, rs1, rs2, rs3  // Variant 1 : rdp = (rs1 || rs2) + rs3
    maccu   rdp, rs1            // Variant 2 : rdp =  rdp         + rs3
\end{cryptoisa}

\begin{lstlisting}[]
// Equivalent RV32IM / RV64IM assembly code listing. rdp = (rd2,rd1)
add     rd1, rs2, rs3
sltu    rs1, rd1, rs2
add     rd2, rs2, rs1   // (rd1,rd2) = (rs1,rs2)+rs3
\end{lstlisting}

Variant 1 of the \mnemonic{maccu} instruction
concatenates source registers
{\tt rd2}
and
{\tt rd1}
to create a $2*$\XLEN word.
To this,
{\tt rs3} is added,
and the full double-\XLEN result is written back to
an odd/even register pair {\tt rdp}.

Variant 2 does the same, but uses {\tt rdp} as an operand and is
destructive.

\note{This instruction is designed to be used in conjunction with
\mnemonic{mmulu}.}

% ============================================================================

\subsection{Lightweight AES Acceleration}

\note{The AES SBox operations are very popular targets for
power or EM based side-channel attacks.
While an instruction like this allows for very efficient AES
implementations, it is very difficult to implement the operation
in a countermeasure friendly way.}

\question{What should the Task Group policy be on enabling
software and/or hardware based side-channel countermeasure?}

\question{Given that enabling and optimising side-channel countermeasures 
is so hard, should the TG focus instead on code-density and performance?
In this case, can implementations with much more structured
inputs (as in \cite{TG:06}) be considered.}

\subsubsection{Variant 1}

\begin{cryptoisa}
RV32, RV64:
    saes.sbenc rd, rs1
    saes.sbdec rd, rs1
\end{cryptoisa}

These instructions implement the 
{\tt SubBytes} \cite[Section 5.1.1]{nist:fips:197}
and
{\tt InvSubBytes} \cite[Section 5.3.1]{nist:fips:197}
steps of the AES Block Cipher \cite{nist:fips:197}.
The low 32-bits of {\tt rs1} are split into bytes.
Each byte has the relevant transformation applied, before
being written back to the corresponding byte position in {\tt rd}.
On an RV64 platform, the high 32-bits of the result are zero
extended.
Psueudo code is found in figure
\ref{fig:pseudo:aes:v1:sub}.

\begin{figure}
\begin{lstlisting}
saes.sbenc(rs1):
    rd.8[0] =    AESSBox[rs1.8[0]], rd.8[1] =    AESSBox[rs1.8[1]]
    rd.8[2] =    AESSBox[rs1.8[2]], rd.8[3] =    AESSBox[rs1.8[3]]

saes.sbdec(rs1):
    rd.8[0] = InvAESSBox[rs1.8[0]], rd.8[1] = InvAESSBox[rs1.8[1]]
    rd.8[2] = InvAESSBox[rs1.8[2]], rd.8[3] = InvAESSBox[rs1.8[3]]
\end{lstlisting}
\caption{AES instruction variant 1: sbox instruction pseudo code.}
\label{fig:pseudo:aes:v1:sub}
\end{figure}

\subsubsection{Variant 2}

\begin{cryptoisa}
RV32, RV64:
    saes.sub.enc    rd, rs1, rs2 // mode = enc, rot = 0
    saes.sub.encr   rd, rs1, rs2 // mode = enc, rot = 1
    saes.sub.dec    rd, rs1, rs2 // mode = dec, rot = 0
    saes.sub.decr   rd, rs1, rs2 // mode = dec, rot = 1

    saes.mix.enc    rd, rs1, rs2 // mode = enc
    saes.mix.dec    rd, rs1, rs2 // mode = dec
\end{cryptoisa}

These instructions are derived from \cite{MPP:19}, which in turn adapted
them originally from \cite{TG:06}.

Pseudo-code for the sub-bytes and mix-columns instructions are found in
figures
\ref{fig:pesudo:aes:v2:sub}
and
\ref{fig:pesudo:aes:v2:mix}
respectivley.

\begin{figure}
\begin{lstlisting}
saes.sub(rs1, rs2, mode, rot):
    if(mode == enc)
        t0 =    AESSBox[rs1.8[0]], t1 =    AESSBox[rs2.8[1]]
        t2 =    AESSBox[rs1.8[2]], t3 =    AESSBox[rs2.8[3]]
    else
        t0 = InvAESSBox[rs1.8[0]], t1 = InvAESSBox[rs2.8[1]]
        t2 = InvAESSBox[rs1.8[2]], t3 = InvAESSBox[rs2.8[3]]
    rd.32 = {t3, t2, t1, t0} (if rot = 0) else {t2, t1, t0, t3}
\end{lstlisting}
\caption{AES instruction variant 2: sbox instruction pseudo code.}
\label{fig:pesudo:aes:v2:sub}
\end{figure}

\begin{figure}
\begin{lstlisting}
saes.mix(rs1, rs2, mode):
    t0 = rs1.8[0], t1 = rs1.8[1]
    t2 = rs2.8[2], t3 = rs2.8[3]
    if(mode == enc)
        rd.32 =    AESMixColumns(t3,t2,t1,t0)
    else
        rd.32 = InvAESMixColumns(t3,t2,t1,t0)
\end{lstlisting}
\caption{AES instruction variant 2: Mix columns instruction pseudo code.}
\label{fig:pesudo:aes:v2:mix}
\end{figure}

\subsubsection{Variant 3}

\begin{cryptoisa}
RV32, RV64:
    saes.ks     rd, rs1, rs2, fa, fb    
    saes.enc    rd, rs1, rs2, fa, fc
    saes.dec    rd, rs1, rs2, fa, fc
\end{cryptoisa}

These instructions are a very lightweight proposal, derived from
\cite{MJS:20}.
In contrast to variants 1 and 2, they perform only a single (Inverse) SBox
lookup per instruction.
Pseudo code is available in figures
\ref{fig:pesudo:aes:v3:ks},
\ref{fig:pesudo:aes:v3:enc} and
\ref{fig:pesudo:aes:v3:dec}.

Note that the input immediate values {\tt fa},{\tt fb} and {\tt fc}
are $2$, $2$ and $1$ bits wide respectively.

\begin{figure}
\begin{lstlisting}
saes.ks(rs1, rs2, fa, fb):
    t0.8  = rs1.8[fa]
     x.8  = AESSBox[tmp]
    u.32  = {0, 0, 0, x }
    rd.32 = ROTL32(u.32, 8*fb) ^ rs2.32
\end{lstlisting}
\caption{AES instruction variant 3: Key schedule instruction pseudo code.}
\label{fig:pesudo:aes:v3:ks}
\end{figure}

\begin{figure}
\begin{lstlisting}
saes.enc(rs1, rs2, fa, fc):
    t0.8 = rs1.8[fa]
    x.8  = AESSBox[tmp]
    x2.8 = (x << 1) ^ ((x & 0x8) ? 0x1B : 0x00)
    if(fc)
        u.32 = {x ^ x2 , x , x , x2}
    else
        u.32 = {     0 , 0 , 0 , x }
    rd.32 = ROTL32(u.32, 8*fa) ^ rs2.32
\end{lstlisting}
\caption{AES instruction variant 3: Encrypt instruction pseudo code.}
\label{fig:pesudo:aes:v3:enc}
\end{figure}

\begin{figure}
\begin{lstlisting}
mulx(x) : (x << 1) ^ ((x & 0x8) ? 0x1B : 0x00)

saes.dec(rs1, rs2, fa, fc):
    t0.8 = rs1.8[fa]
    x.8  = INVAESSBox[tmp]
    x2.8 = mulx( x.8), x4.8 = mulx(x2.8), x8.8 = mulx(x4.8)
    if(fc)
        u.32 = {x ^ x2 ^ x4 , x ^ x4 ^ x8 , x ^ x8 , x2 ^ x4 ^ x8}
    else
        u.32 = {          0 ,           0 ,      0 ,           x }
    rd.32 = ROTL32(u.32, 8*fa) ^ rs2.32
\end{lstlisting}
\caption{AES instruction variant 3: Decrypt instruction pseudo code.}
\label{fig:pesudo:aes:v3:dec}
\end{figure}

% ============================================================================

\subsection{Lightweight SHA2 Acceleration}

\begin{cryptoisa}
RV32, RV64:
    ssha256.s0 rd, rs1 : rd = ror32(rs1, 7) ^ ror32(rs1, 18) ^ srl32(rs1, 3)
    ssha256.s1 rd, rs1 : rd = ror32(rs1,17) ^ ror32(rs1, 19) ^ srl32(rs1,10)
    ssha256.s2 rd, rs1 : rd = ror32(rs1, 2) ^ ror32(rs1, 13) ^ ror32(rs1,22)
    ssha256.s3 rd, rs1 : rd = ror32(rs1, 6) ^ ror32(rs1, 11) ^ ror32(rs1,25)
\end{cryptoisa}

The {\tt ssha256.sX}
instructions implement the core of the four sigma and sum functions used in
the SHA256 hash function \cite[Section 4.1.2]{nist:fips:180:4}.
These operations will be supported for a both RV32 and RV64 targets.
For RV32, the entire XLEN source register is operated on.
For RV64, the low 32-bits of the XLEN register are read and operated on,
with the result zero extended to XLEN bits.
Though named for SHA256, the instructions work for both the
SHA-224 and SHA-256 parameterisations as described in
\cite{nist:fips:180:4}.


\begin{cryptoisa}
RV64:
    ssha512.s0 rd, rs1 : rd = ror64(rs1, 1) ^ ror64(rs1,  8) ^ srl64(rs1, 7)
    ssha512.s1 rd, rs1 : rd = ror64(rs1,19) ^ ror64(rs1, 61) ^ srl64(rs1, 6)
    ssha512.s2 rd, rs1 : rd = ror64(rs1,28) ^ ror64(rs1, 34) ^ ror64(rs1,39)
    ssha512.s3 rd, rs1 : rd = ror64(rs1,14) ^ ror64(rs1, 18) ^ ror64(rs1,41)
\end{cryptoisa}

The \mnemonic{ssha512.sX}
instructions implement the core of the four sigma and sum functions used in
the SHA512 hash function \cite[Section 4.1.3]{nist:fips:180:4}.
These operations will be supported for RV64 targets only.
Though named for the SHA-512 parameterisation, the instructions
can be used for all of the SHA-384, SHA-512, SHA-512/224 and SHA-512/256
parameterisations as described in \cite{nist:fips:180:4}.

\begin{table}[]
\centering
\begin{tabular}{lll}
Architecture      & Static Code Size (Bytes) & Instructions Executed \\ \hline
rv32gc            & 14934                    & 78003                 \\
rv32gcb           & 10086                    & 56866 (1.37x)         \\
rv32gcb\_Zscrypto & 5938                     & 28539 (2.73x)
\end{tabular}
\caption{Static code size and instructions executed comparison for
the \mnemonic{ssha256.sx} instructions on RV32 based architectures.
Instruction execution counts are for hashing 1024 bytes of data.}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{lll}
Architecture      & Static Code Size (Bytes) & Instructions Executed \\ \hline
rv64gc            & 20490                    & 73138                 \\
rv64gcb           & 14216                    & 53153 (1.38x)         \\
rv64gcb\_Zscrypto & 8954                     & 35881 (2.04x)
\end{tabular}
\caption{Static code size and instructions executed comparison for
the \mnemonic{ssha512.sx} instructions on RV64 based architectures.
Instruction execution counts are for hashing 1024 bytes of data.}
\end{table}


\note{
The remaining two core functions which make up the SHA256/512
hash functions are the $Ch$ and $Maj$ functions:
\begin{itemize}
\item \lstinline{Ch(x,y,z)  = (x & y) ^ (~x & z)}
\item \lstinline{Maj(x,y,z) = (x & y) ^ ( x & z) ^ ( y & z )}
\end{itemize}
As ternary functions, they are much too expensive in terms of
opcode space to consider for inclusion as dedicated instructions for
such a specialist use case.
They are amenable however to macro-op fusion on cores which implement it.
}

% ============================================================================

\subsection{Lightweight SHA3 Acceleration}
\label{sec:scalar:sha3}

\begin{cryptoisa}
RV32, RV64:
    ssha3.xy rd, rs1, rs2 : rd = (( rs1    % 5) + 5(           rs2 % 5)) << 3
    ssha3.x1 rd, rs1, rs2 : rd = (((rs1+1) % 5) + 5(           rs2 % 5)) << 3
    ssha3.x2 rd, rs1, rs2 : rd = (((rs1+2) % 5) + 5(           rs2 % 5)) << 3
    ssha3.x4 rd, rs1, rs2 : rd = (((rs1+4) % 5) + 5(           rs2 % 5)) << 3
    ssha3.yx rd, rs1, rs2 : rd = (( rs2    % 5) + 5((2*rs1 + 3*rs2)% 5)) << 3
\end{cryptoisa}

These instructions accelerate code-dense implementations of the SHA3 secure
hash function \cite{nist:fips:202}.
They work on the low $3$ bits of the input {\tt rs1} and {\tt rs2} registers,
and compute indices into the state array of the round function.
They are designed to replace both canonical implementations of the
{\tt index} function and lookup table based implementations.
In both cases, these instructions offer substantial
improvements in performance, static code size and dynamic instruction
bandwidth.
A longer discussion of the merits of these instructions can
be found in Appendix \ref{sec:appendix:sha3}.

\note{
SHA3 is not yet widely used, especially with respect to SHA2.
These instructions enable implementations with a much better
performance/code-density ratio than is currently possible in RISC-V.
}

\note{
If the auto-aligning indexed load and store instructions are included,
then the auto-aligning component of these instructions
may not be needed.
}

% ============================================================================

\subsection{Auto-aligning Indexed Load and Store}
\label{sec:ildst}

\begin{cryptoisa}
RV32, RV64:
    lbx     rd,  rs1, rs2 : rd = sext(mem[rs1 + (rs2     )])
    lbux    rd,  rs1, rs2 : rd = zext(mem[rs1 + (rs2     )])
    lhx     rd,  rs1, rs2 : rd = sext(mem[rs1 + (rs2 << 1)])
    lhux    rd,  rs1, rs2 : rd = zext(mem[rs1 + (rs2 << 1)])
    lwx     rd,  rs1, rs2 : rd = sext(mem[rs1 + (rs2 << 2)])
    sbx     rs1, rs2, rs3 : mem[rs1 + (rs3     )] = rs2
    shx     rs1, rs2, rs3 : mem[rs1 + (rs3 << 1)] = rs2
    swx     rs1, rs2, rs3 : mem[rs1 + (rs3 << 2)] = rs2

RV64:
    lwux    rd,  rs1, rs2 : rd = zext(mem[rs1 + (rs2 << 2)])
    ldx     rd,  rs1, rs2 : rd = sext(mem[rs1 + (rs2 << 3)])
    sdx     rs1, rs2, rs3 : mem[rs1 + (rs3 << 3)] = rs2
\end{cryptoisa}

\note{
There is a longer discussion about the inclusion of indexed load
and store in the context of RISC-V in
Appendix \ref{sec:appendix:ildst}.
}

These instructions add indexed load and store functionality to RISC-V.
The load instructions take a base register {\tt rs1} and an offset
register {\tt rs2} to form an address.
The offset is aligned to the data type of the instruction.
Zero and sign extending variants are provided.
The store instructions do the same to form the effective address, but
use {\tt rs3} as the auto-aligned offset.
This pattern of register usage keeps the functions of {\tt rs1} and
{\tt rs2} the same as the immediate offset load and store instructions.

Indexed load and store are useful for implementing many cryptographic
algorithms.
For public key cryptography involving modular exponentiation, iterating
independently over several arrays without lots of pointer arithmetic
is beneficial.
For hash functions and symmetric key block ciphers, irregular or
non-sequential access to a state array also benefits from
being able to directly calculate an address based on an offset in a
register.

% ============================================================================

\subsection{Micro-architectural Recommendations}

\todo{Macro-op fusion suggestions, side-channel considerations.}

% ============================================================================

