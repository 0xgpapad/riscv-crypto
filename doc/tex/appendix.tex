
\section{Benchmarking}

\subsection{AES}
\label{sec:benchmark:aes}

The source code for these benchmarks is available as part of the
{\tt riscv-crypto} 
\href{https://github.com/scarv/riscv-crypto/tree/master/benchmarks/crypto_block/aes}{GitHub repository}.

\begin{table}[h]
\centering
\begin{tabular}{lrrrrr}
AES Proposal & Instrs Enc & Instrs Dec & Instrs Ks Enc & Instrs Ks Dec & Static Code Size (Bytes) \\ \hline
Reference   & 3260  & 6704  & 594  &  594  & 6233   \\
TTable      & 1005  & 1012  & 625  &  1863 & 14353  \\
V1 Latency  & 2756  & 6537  & 462  &  462  & 3575   \\
V1 Size     & 2874  & 6681  & 510  &  510  & -      \\
V2 Latency  & 325   & 307   & 462  &  581  & 1867   \\
V2 Size     & 552   & 539   & 462  &  687  & -      \\
V3.1        & 321   & 321   & 238  &  682  & 1952   \\
V3.2        & 480   & 470   & 238  &  826  & 2264   \\
V4 / RV64   & TBD   & TBD   & TBD  &  TBD  & TBD     \\
\end{tabular}
\caption{
AES instruction execution counts measured using Spike
running {\tt rv32imcb\_Zscrypto}.
Instruction counts are for a single AES 128 block encrypt/decrypt operation,
or for a single encrypt/decrypt key schedule.
V1 and V2 proposals include results for ``latency" optimised and ``size"
optimised implementations of the underlying instructions.
The ``latency" rows assume one cycle per instruction.
The ``size" rows assume four cycles per instruction (corresponding to
one implemented SBox), which is modelled by padding each AES instruction
with 3 additional {\tt nop} instructions.
Static code size is reported as the sum of all {\tt .text} and {\tt .data}
sections.
}
\label{tab:benchmarks:aes:perf}
\end{table}


\begin{table}[h]
\centering
\begin{tabular}{llllllll}
AES Proposal & Longest Topological Path & NAND2 Gates \\ \hline
V1 Latency   & 18                       & 2376        \\
V1 Size      & 21                       & 966         \\
V2 Latency   & 18                       & 3453        \\
V2 Size      & 21                       & 1287        \\
V3.1         & 30                       & 1157        \\
V3.2         & 23                       & 1117        \\
V4 / RV64 Latency & 27                  & 8027        \\
\end{tabular}
\caption{
AES proposal RTL implementation benchmarks.
Measured using the Yosys simple CMOS flow.
The V1 and V2 proposals have "Latency" and "Size" optimised versions.
The "Latency" versions use $4$ SBox instantiations and compute their
results in a single clock cycle.
The "Size" versions use $1$ SBox instantiation, and compute their
results over 4 clock cycles.
}
\label{tab:benchmarks:aes:impl}
\end{table}

\newpage
\subsection{SHA2}
\label{sec:benchmark:sha2}

The source code for the
SHA256\footnote{\url{https://github.com/scarv/riscv-crypto/tree/master/benchmarks/crypto_hash/sha256}}
and
SHA512\footnote{\url{https://github.com/scarv/riscv-crypto/tree/master/benchmarks/crypto_hash/sha512}}
benchmarks are available as part of the {\tt riscv-crypto}
\href{https://github.com/scarv/riscv-crypto/tree/master/benchmarks/crypto_block/aes}{GitHub repository}.

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
Architecture      & Static Code Size (Bytes) & Instructions Executed & Performance Gain \\ \hline
rv32gc            & 14934                    & 78003 & 1.00x          \\
rv32gcb           & 10086                    & 56866 & 1.37x          \\
rv32gcb\_Zscrypto & 5938                     & 28539 & 2.73x 
\end{tabular}
\caption{{\bf SHA256:}
Static code size and instructions executed comparison for
the \mnemonic{ssha256.sx} instructions on RV32 based architectures.
Instruction execution counts are for hashing 1024 bytes of data using
SHA256.
}
\label{tab:benchmarks:sha256}
\end{table}

Area and path length estimates are calculated using a basic Yosys CMOS
synthesis flow.

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
Architecture      & Static Code Size (Bytes) & Instructions Executed & Performance Gain \\ \hline
rv64gc            & 20490                    & 73138 & 1.00x          \\
rv64gcb           & 14216                    & 53153 & 1.38x          \\
rv64gcb\_Zscrypto & 8954                     & 35881 & 2.04x 
\end{tabular}
\caption{{\bf SHA512:}
Static code size and instructions executed comparison for
the \mnemonic{ssha512.sx} instructions on RV64 based architectures.
Instruction execution counts are for hashing 1024 bytes of data
using SHA512.
}
\label{tab:benchmarks:sha512}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lrr}
Instructions   & Longest Topological Path & Area (NAND2 Equivalent) \\ \hline
ssha256.s*     & 5                        & 787                   \\
ssha512.s*     & 6                        & 1534                  \\
\end{tabular}
\caption{
Area and path length estimates are calculated using a basic Yosys CMOS
synthesis flow.
}
\label{tab:benchmarks:sha2:rtl}
\end{table}


% ============================================================================

\section{Rejected AES Proposals}

These proposals were considered and benchmarked as possible AES instruction
candidates.
Their details are left here for posterity.
The proposals which have been taken forward were referred too as
``v3.1" for the 32-bit variant, and ``v4" for the 64-bit variant.

% ------------------------------------------------------------

\subsection{Variant 1 (RV32,RV64)}

\begin{cryptoisa}
RV32, RV64:
    saes.v1.enc rd, rs1
    saes.v1.dec rd, rs1
\end{cryptoisa}

These instructions implement the 
{\tt SubBytes} \cite[Section 5.1.1]{nist:fips:197}
and
{\tt InvSubBytes} \cite[Section 5.3.1]{nist:fips:197}
steps of the AES Block Cipher \cite{nist:fips:197}.
The low 32-bits of {\tt rs1} are split into bytes.
Each byte has the relevant transformation applied, before
being written back to the corresponding byte position in {\tt rd}.
On an RV64 platform, the high 32-bits of the result are zero
extended.
Pseudo code is found in 
\figref{pseudo:aes:v1}.

This proposal variant requires $2$ encoding points with only one
source register.

\begin{figure}
\begin{subfigure}[b]{0.5\textwidth}
\begin{lstlisting}[language=pseudo]
saes.v1.enc(rs1):
    rd.8[0] =    AESSBox(rs1.8[0])
    rd.8[1] =    AESSBox(rs1.8[1])
    rd.8[2] =    AESSBox(rs1.8[2])
    rd.8[3] =    AESSBox(rs1.8[3])
\end{lstlisting}
\caption{Forward SBox instruction pseudo code.}
\label{fig:pseudo:aes:v1:sub:enc}
\end{subfigure}
\begin{subfigure}[b]{0.5\textwidth}
\begin{lstlisting}[language=pseudo]
saes.v1.dec(rs1):
    rd.8[0] = InvAESSBox(rs1.8[0])
    rd.8[1] = InvAESSBox(rs1.8[1])
    rd.8[2] = InvAESSBox(rs1.8[2])
    rd.8[3] = InvAESSBox(rs1.8[3])
\end{lstlisting}
\label{fig:pseudo:aes:v1:sub:dec}
\caption{Inverse SBox instruction pseudo code.}
\end{subfigure}
\caption{AES Instructions variant 1.}
\label{fig:pseudo:aes:v1}.
\end{figure}

% ------------------------------------------------------------
\newpage
\subsection{Variant 2 (RV32,RV64)}

\begin{cryptoisa}
RV32, RV64:
    saes.v2.sub.enc    rd, rs1, rs2
    saes.v2.sub.dec    rd, rs1, rs2
    saes.v2.mix.enc    rd, rs1, rs2
    saes.v2.mix.dec    rd, rs1, rs2
\end{cryptoisa}

These instructions are derived from \cite{MPP:19}, which in turn adapted
them originally from \cite{TG:06}.
Each instruction performs either the SubBytes or MixColumns transformation
to a single column of the AES state, along with a partial ShiftRows.
Example usage can be found in the riscv-crypto repository
\footnote{\url{https://github.com/scarv/riscv-crypto/blob/master/benchmarks/crypto_block/aes/zscrypto_v2/aes_enc.c\#L53}}.

Pseudo-code for the sub-bytes and mix-columns instructions are found in
\figref{pesudo:aes:v2:sub}
and
\figref{pesudo:aes:v2:mix}
respectively.

This proposal variant requires $4$ encoding points, where each point
requires two source registers and a destination register.

\begin{figure}
\begin{subfigure}[b]{1.0\textwidth}
\begin{lstlisting}[language=pseudo]
saes.v2.sub(rs1, rs2, mode):
    if(mode == enc)
        t0 =    AESSBox(rs1.8[0]), t1 =    AESSBox(rs2.8[1])
        t2 =    AESSBox(rs1.8[2]), t3 =    AESSBox(rs2.8[3])
    else
        t0 = InvAESSBox(rs1.8[0]), t1 = InvAESSBox(rs2.8[1])
        t2 = InvAESSBox(rs1.8[2]), t3 = InvAESSBox(rs2.8[3])
    rd.32 = {t3, t2, t1, t0} 
\end{lstlisting}
\caption{AES instruction variant 2: sbox instruction pseudo code.}
\label{fig:pesudo:aes:v2:sub}
\end{subfigure}
\begin{subfigure}[b]{1.0\textwidth}
\begin{lstlisting}[language=pseudo]
saes.v2.mix(rs1, rs2, mode):
    t0 = rs1.8[0], t1 = rs1.8[1]
    t2 = rs2.8[2], t3 = rs2.8[3]
    if(mode == enc)
        rd.32 =    AESMixColumns(t3,t2,t1,t0)
    else
        rd.32 = InvAESMixColumns(t3,t2,t1,t0)
\end{lstlisting}
\caption{AES instruction variant 2: Mix columns instruction pseudo code.}
\label{fig:pesudo:aes:v2:mix}
\end{subfigure}
\caption{AES Instructions: Variant 2 Pseudo Code.}
\end{figure}

% ------------------------------------------------------------


% ============================================================================


\section{Indexed Load and Store Discussion}
\label{sec:appendix:ildst}

\note{
The following section is included to stimulate discussion.
There are good engineering arguments for and against
indexed load and store in the context of RISC-V which this
section aims to capture.
}

RISC-V very deliberately omits indexed load and store instructions
from the base architecture \cite{CDPA:16}.
The principle arguments for this are:

\begin{itemize}
\item That the extra register read port for the 3-operand store instructions
    are an unacceptable burden on smaller micro-architectures.
\item That macro-op fusion is a sufficient mitigation for the performance
    penalty.
    In \cite[Sections V, VI]{CDPA:16}, the authors use the example of
    indexed load to demonstrate this.
    Their results using the SPECInt benchmarks are very encouraging.
\end{itemize}

With the proposed inclusion of ternary instructions as part of Bitmanip, the
first argument no longer holds as much (or any) weight for CPUs which are
implementing many/any of the ternary instructions anyway.

The second argument then requires further discussion.
Note that we do not argue against macro-op fusion in general, it
is still a useful technique for optimising instruction execution
and maintaining ISA cleanliness \footnote{For some definition thereof.}.
In the case of indexed load and store however, it's merits are
less certain, both generally, and in the case of RISC-V specifically.

\begin{itemize}
\item For narrow memory bus widths (32-bits), one can only
    fuse adjacent 16-bit opcodes. This can be mitigated with an
    instruction fetch buffer and the associated costs this brings.

\item In \cite{CDPA:16}, the authors focus on fusing opcodes from the
    compressed ISA. While this makes sense, it severely limits the
    registers available due to the limited addressing capabilities of
    the RVC instructions.

\item There is no reason 32-bit opcodes cannot be fused, but this
    implies much wider memory busses, deeper fetch buffers, or both.

\item It is an open question how a compiler tuned to generate
    fusion pairs will interact with a core which doesn't implement
    fusion. Intuitively, this may mean activating forwarding paths
    much more often. This may result in more toggling and hence
    energy consumption. We could find no empirical evidence one way
    or the other on this.

\item The recommended fusable sequence for an auto-aligning indexed load in
    \cite[Section VI.A]{CDPA:16} is three 16-bit compressed instructions.
    This is a criterion used by the Bitmanip extension to measure whether
    instructions are worthy of inclusion: does it replace three
    instructions, or two instructions and is very commonly used.
    We would argue that an auto-aligning indexed load and store
    meets both of these.

\item A fused store instruction sequence requires that the calculated
    address be written back to the GPRs, even if the address is never
    used again.

\item Certain bus standards (AMBA AHB, AXI) which have separate address and
    data phases or channels, possibly removing the need for indexed stores to
    access all three operands simultaneously.
    This implies a possible performance penalty.

\item While these instructions are very useful generally, they have
    particular usefulness in Cryptography.
    In the case of public key cryptography, one typically needs to do
    large amounts of multi-precision arithmetic.
    Basic schemes for this rely on iterating over at-least three
    different arrays with different indices, for which indexed load
    and store are very useful.
    More complex modular exponentiation schemes iterate less regularly
    over the input/output arrays, making the pointer arithmetic involved
    even more burdensome.
    In block ciphers and hash functions which are not loop-unrolled,
    non-sequential access to a state array is also a common idiom.

\item Given the results in \cite{CDPA:16}, it is clear that
    macro-op fusion is a good scheme for enhancing implementations of
    RISC-V.
    It does not offer a comparison of performance based on a
    hypothetical extension to RISC-V which does include
    auto-aligning indexed load and store.
    This work and associated results are essential for a full discussion on
    the matter.
\end{itemize}

\todo{
Repeat the work of \cite{CDPA:16}, but include a version of
RISC-V with the auto-aligning instructions described
in section \ref{sec:ildst}.
}

% ============================================================================

\newpage
\section{Equivalent C-Code Listings}
\label{sec:ccode}

\subsection{LUT4}
\label{sec:ccode:lut4}

\begin{figure}[h]
\begin{lstlisting}[style=C]
uint32_t lut4_lo (uint32_t rs1, uint32_t rs2) {
    uint32_t result = 0;
    for(int i = 0; i < 32; i += 4) {
        uint8_t idx =  (rs1 >> i) & 0x7;
        uint8_t lo  = ((rs1 >> i) & 0xF) < 8;
        if(lo) { result |= ((rs2 >> (4*idx) & 0xF) << i; }
    }
    return result;
}
uint32_t lut4_hi (uint32_t rs1, uint32_t rs2) {
    uint32_t result = 0;
    for(int i = 0; i < 32; i += 4) {
        uint8_t idx =  (rs1 >> i) & 0x7;
        uint8_t hi  = ((rs1 >> i) & 0xF) > 8;
        if(hi) { result |= ((rs2 >> (4*idx) & 0xF) << i; }
    }
    return result;
}
uint64_t lut4 (uint64_t rs1, uint64_t rs2) {
    uint64_t result = 0;
    for(int i = 0; i < 64; i += 4) {
        result |= ((rs2 >> (4*((rs1 >> i)&0xF)) & 0xF) << i;
    }
    return result;
}
\end{lstlisting}
\caption{
    Equivalent C code models for the
    \mnemonic{lut4lo}, \mnemonic{lut4hi} and \mnemonic{lut4}
    instructions.
}
\label{fig:equiv:c:lut4}
\end{figure}

% ============================================================================

\newpage
\section{Tentative Encoding Proposals}

% NOTE: These are generated by the `bin/parse_opcodes.py` script,
% which is called by $REPO_HOME/doc/Makefile

\import{../build/}{opcodes-crypto.tex}


