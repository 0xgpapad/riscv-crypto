
\section{SHA3 Instruction Discussions}
\label{sec:appendix:sha3}

This discussion follows on from the instruction specifications
in section
\ref{sec:scalar:sha3}.

The SHA3 secure hash function \cite{nist:fips:202} is based on
the KECCAK-P family of permutations.
SHA3 is notably slower than SHA2 when implemented in software.
It also has a large state size (1600 bits) which is very irregularly
accessed, making it difficult to accelerate
the core {\em compute} operations as part of a scalar CPU pipeline.
We distinguish between {\em compute} operations (which modify the
round function state) and {\em address} operations (which calculate
indexes into the round function state) when motivating these instructions.

The core operations of the KECCAK-P round function are rotations
and XORs, which are already well supported by the RISC-V
base and Bitmanip architectures.
The round function state is accessed as a $5*5$ array of
64-bit words.
See Figure \ref{fig:listing:sha3} for a C code implementation of
the core KeccakP1600 round function which SHA3 depends on.
When developing lightweight accelerator instructions for SHA3, we
consider two broad implementation options:

\begin{itemize}
\item Loop-unrolled: Here, all of the loops of the round function are
    unrolled, meaning that all variations of the \lstinline{index}
    function are computed at compile time, and are emitted as immediate
    offsets to load and store instructions.
    In this case, there is little that can be added to a scalar
    pipeline to accelerate SHA3, other than the bitwise rotation instructions
    (for RV64) or funnel shift instructions (for RV32).
    The proposed instructions do not benefit a loop-unrolled implementation.
\item Loop-rolled-up: The loops are not unrolled, and the
    \lstinline{index} functions are re-computed on every loop iteration.
    This means that {\em either} {\tt rem} instructions are used to
    compute the modulo $5$ operations, or they can be replaced with a
    lookup table.
    In both cases, the extra number of instructions executed is
    substantial.
    The proposed instructions strongly benefit a loop-rolled-up
    implementation.
    This has obvious benefits for embedded applications where code-density
    is an issue.
    It may also benefit larger implementations due to better use of
    instruction caches, especially in the presence of a good branch
    predictor.
\end{itemize}

The proposed instructions are {\em extremely} small to implement, since
they only read the low $3$ bits of the two source registers, and
the result is only ever $5$ bits wide prior to double-word alignment and
$8$ bits afterwards.
An example implementation
\footnote{\url{https://github.com/scarv/xcrypto/blob/dev/ben/issue-73/rtl/xc_sha3/xc_sha3.v}}
synthesised to a simple CMOS cell library using Yosys comes to
approximately $300$ cells.
Note that this is only the cost of dedicated instruction logic.
Decode costs are not included.

\begin{table}[]
\centering
\begin{tabular}{llllll}
Architecture & Flags & {\tt .text} Bytes & Instructions Executed & Fetch Bandwidth & Data Bandwidth \\ \hline
{\tt rv32im } & -O2 &  688 &  & & \\
{\tt rv32imc} & -O2 &  558 &  & & \\
{\tt rv64im } & -O2 &  504 &  & & \\
{\tt rv64imc} & -O2 &  354 &  & & \\
{\tt rv32im } & -O3 & 3328 &  & & \\
{\tt rv32imc} & -O3 & 2812 &  & & \\
{\tt rv64im } & -O3 & 1292 &  & & \\
{\tt rv64imc} & -O3 & 1034 &  & & \\
\end{tabular}
\caption{Table of code size and performance comparisons for the SHA3
algorithm, implemented on various RISC-V architecture variants.
}
\end{table}

\todo{
Benchmarking flow for SHA3 and the KeccakP1600 round function:
Code size, performance, instructions executed, instruction bandwidth,
data bandwidth etc.
}


\begin{figure}
\lstinputlisting[language=c]{../benchmarks/hash/sha3/keccakp1600.c}
\caption{A C code implementation of the KeccakP1600 permutation, as
used by the SHA3 secure hash function.}
\label{fig:listing:sha3}
\end{figure}

% ============================================================================


\section{Indexed Load and Store Discussion}
\label{sec:appendix:ildst}

\note{
The following section is included to stimulate discussion.
There are good engineering arguments for and against
indexed load and store in the context of RISC-V which this
section aims to capture.
}

RISC-V very deliberately omits indexed load and store instructions
from the base architecture \cite{CDPA:16}.
The principle arguments for this are:

\begin{itemize}
\item That the extra register read port for the 3-operand store instructions
    are an unacceptable burden on smaller micro-architectures.
\item That macro-op fusion is a sufficient mitigation for the performance
    penalty.
    In \cite[Sections V, VI]{CDPA:16}, the authors use the example of
    indexed load to demonstrate this.
    Their results using the SPECInt benchmarks are very encouraging.
\end{itemize}

With the proposed inclusion of ternary instructions as part of Bitmanip, the
first argument no longer holds as much (or any) weight for CPUs which are
implementing many/any of the ternary instructions anyway.

The second argument then requires further discussion.
Note that we do not argue against macro-op fusion in general, it
is still a useful technique for optimising instruction execution
and maintaining ISA cleanliness \footnote{For some definition thereof.}.
In the case of indexed load and store however, it's merits are
less certain, both generally, and in the case of RISC-V specifically.

\begin{itemize}
\item For narrow memory bus widths (32-bits), one can only
    fuse adjacent 16-bit opcodes. This can be mitigated with an
    instruction fetch buffer and the associated costs this brings.

\item In \cite{CDPA:16}, the authors focus on fusing opcodes from the
    compressed ISA. While this makes sense, it severely limits the
    registers available due to the limited addressing capabilities of
    the RVC instructions.

\item There is no reason 32-bit opcodes cannot be fused, but this
    implies much wider memory busses, deeper fetch buffers, or both.

\item It is an open question how a compiler tuned to generate
    fusion pairs will interact with a core which doesn't implement
    fusion. Intuitively, this may mean activating forwarding paths
    much more often. This may result in more toggling and hence
    energy consumption. We could find no empirical evidence one way
    or the other on this.

\item The recommended fusable sequence for an auto-aligning indexed load in
    \cite[Section VI.A]{CDPA:16} is three 16-bit compressed instructions.
    This is a criterion used by the Bitmanip extension to measure whether
    instructions are worthy of inclusion: does it replace three
    instructions, or two instructions and is very commonly used.
    We would argue that an auto-aligning indexed load and store
    meets both of these.

\item A fused store instruction sequence requires that the calculated
    address be written back to the GPRs, even if the address is never
    used again.

\item Certain bus standards (AMBA AHB, AXI) which have separate address and
    data phases or channels, possibly removing the need for indexed stores to
    access all three operands simultaneously.
    This implies a possible performance penalty.

\item While these instructions are very useful generally, they have
    particular usefulness in Cryptography.
    In the case of public key cryptography, one typically needs to do
    large amounts of multi-precision arithmetic.
    Basic schemes for this rely on iterating over at-least three
    different arrays with different indices, for which indexed load
    and store are very useful.
    More complex modular exponentiation schemes iterate less regularly
    over the input/output arrays, making the pointer arithmetic involved
    even more burdensome.
    In block ciphers and hash functions which are not loop-unrolled,
    non-sequential access to a state array is also a common idiom.

\item Given the results in \cite{CDPA:16}, it is clear that
    macro-op fusion is a good scheme for enhancing implementations of
    RISC-V.
    It does not offer a comparison of performance based on a
    hypothetical extension to RISC-V which does include
    auto-aligning indexed load and store.
    This work and associated results are essential for a full discussion on
    the matter.
\end{itemize}

\todo{
Repeat the work of \cite{CDPA:16}, but include a version of
RISC-V with the auto-aligning instructions described
in section \ref{sec:ildst}.
}


% ============================================================================


\section{Tentative Encoding Proposals}

% NOTE: These are generated by the `bin/parse_opcodes.py` script,
% which is called by $REPO_HOME/doc/Makefile

\note{
    All of these encodings are {\em temporary}.
    They have been assigned to enable experimenting with instructions.
    They will change before the standard is submitted for ratification.
}

\todo{
    Fix how parse opcodes script prints the {\tt rdp} field.
    Currently makes it just look like zeros. Should be four bit
    field aligned to top of normal {\tt rd} field.
}

\import{../build/}{opcodes-crypto.tex}


